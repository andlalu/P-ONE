\documentclass[11pt,a4paper,notitlepage]{article}

%usepackage inclusions:
\usepackage{mathtools} %package that allows math formatting
\usepackage{commath} %package that allows for nice differential operators
\usepackage{bigints} %package that allows for bigger integral signs
\usepackage{amsfonts}   %package that allows for more math characters
\usepackage{amssymb}  %package that allows for more math symbols
\usepackage{shadethm}  %package that allows for shaded definitions&proofs
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor} %package that allows colored text
\usepackage{booktabs}
\usepackage{float} % by Jakob
\usepackage[table]{xcolor} %to color table rows & columns
\usepackage{setspace} %to allow for the insertion line spaces at will
\usepackage{wrapfig} %to allow for figures to be captioned
\usepackage{mdframed} % to allow for wide frames and boxes around math&text
\usepackage[makeroom]{cancel} %to allow for strike-out text
\usepackage{subfigure}
\usepackage{parskip}
\usepackage{url} %to allow citing url addresses
\usepackage{rotating}
\usepackage{nameref}
\usepackage{geometry}
\usepackage[margin=1cm]{caption}
\usepackage{changepage}
\usepackage{natbib}

%define own operators&theorems&proofs
\newcommand{\expect}[1]{\mathbb{E} \left[ {#1} \right] } %define expectation symbol 
\newtheorem{proof}{Proof} %define proofs within the shadethm environment
\newcommand{\vect}[1]{\boldsymbol{#1}} %define  BOLD vectors.
\newcommand{\expe}[1]{\mathbb{E}\left[#1\right]} %define  BOLD vectors.
\newcommand{\HRule}{\rule{\linewidth}{0.01mm}} %define horizontal thin line
\newcommand{\red}[1]{{\color{red}{#1}}}

%set page margins
%\addtolength{\textheight}{2cm} 
%\addtolength{\voffset}{-1cm}
%\addtolength{\textwidth}{2cm}
%\addtolength{\hoffset}{-3cm}
\setlength{\oddsidemargin}{0pt} %set left & right margins
\setlength{\evensidemargin}{0pt} %set left & right margins
%set headers and footers
%\usepackage{fancyhdr}  
%\setlength{\headheight}{0pt}
%\pagestyle{fancy}
%\fancyhf{}
\usepackage{indentfirst}
\numberwithin{equation}{section}
%set fonts used
%\usepackage{lmodern}
%\usepackage{kpfonts}
%can check http://www.tug.dk/FontCatalogue/mathfonts.html for font alternatives
%set authoring details
\author{ %name under title %email in footnote \thanks{a.lalu@tinbergen.nl} 
	    }
\title{\vspace*{-55pt} Risk Premium Inference from (Noisy) Option Price Panels \vspace*{-35pt}}
\date{\item First version: June 16, 2018 \item Previous Version : February 21, 2024 \item This version: \today}
%start file content
\begin{document}
\maketitle
%\abstract{
%\tableofcontents
%}

\onehalfspacing

\section*{Brief Description of Research Idea}
Compare the finite sample performance of implied state vs. particle filtering when conducting joint $\mathbb{P}$-$\mathbb{Q}$	estimation on noisy option price panels focusing on the implications this has for recovered (equity) risk premium components and their error distributions/confidence bands. 

\section*{Tentative Outline of Building Blocks}
\begin{enumerate}
\item Implement an efficient simulation scheme for a Heston \citep{heston} state space simulation and associated option price panels.
\item Implement IS estimation \`{a} la \cite{boswijk2015asset}.
\item Implement particle filtering \`{a} la \cite{hurn2015pf}.
\item No noise/best case.
\item Noisy option panel case with an assorted menu of assumptions regarding option price error distributions.
\end{enumerate}

\section*{Rationale}
\begin{enumerate}
\item[A.] Noisy option data is something I had to deal with both for the first two papers, outside academia and having spent some more time looking at post 2018 academic work it seems that this is an emerging topic with a few papers that touch on this in a different context: \cite{andersen2021spatial}, \cite{duarte2019very}.
\item[B.] Topic itself ties in well with the other two papers in my thesis.
\item[C.] I have sufficient background to work on this independently.
\end{enumerate}

\newpage
 
\section{Assumed Data Generating Proces And Option Pricing}
We consider a filtered probability space $\left(\Omega, \mathcal{F}, \lbrace\mathcal{F}\rbrace_{t\geq0}, \mathbb{P} \right)$ satisfying the usual conditions\footnote{See e.g., \citet{protterstochastic}, p. 3.}. Under the historical probability measure, $\mathbb{P}$, the dynamics of the log-price process $\log(S_t)$ and of the stochastic volatility process $V_t$, which make up the state vector $X_t = [\log(S_t), V_t]$, are assumed to be described by the pair of s.d.e.:
\begin{align}
	\dif{\log(S_t)} &= \left((r_t-q_t)+ \left(\eta-1/2\right) V_t \right)\dif t + \sqrt{V_t} \dif W_{t,1}^\mathbb{P}; \label{PHestonS}\\
	\dif{V_t} &= k(\bar{V}-V_t)\dif t + \sigma_v\sqrt{V_t}\left(\rho \dif W_{t,1}^\mathbb{P} + \sqrt{1-\rho^2}\dif W_{t,2}^\mathbb{P}\right). \label{PHestonV}
\end{align}
$\left(\dif W_{t,1}^\mathbb{P}, \dif W_{t,2}^\mathbb{P}\right)$ are $\mathcal{F}_t$-adapted Brownian motion processes. $\lbrace r \rbrace_{t\geq 0}$  and  $\lbrace q \rbrace_{t\geq 0}$ are $\mathcal{F}_t$-adapted processes (e.g., deterministic) which describe the risk-free rate of interest and the continuous dividend yield respectively. $\eta$ is a parameter which governs the size of the equity risk premium. The local stochastic volatility process mean reverts at a mean reversion rate $\kappa$ to the long run mean level $\bar{V}$. $\sigma_v$ governs the volatility of volatility and $\rho$ represents the instantaneous correlation between returns and volatility.

Under the risk neutral probability measure, $\mathbb{Q}$, the model dynamics are assumed to be: 
\begin{align}
	\dif{\log(S_t)} &= \left( (r_t-q_t) - 1/2 V_t\right) \dif t + \sqrt{V_t} \dif W_{t,1}^\mathbb{Q};\label{QHestonS}\\
	\dif{V_t} &= k(\bar{V}-V_t)\dif t + \eta_{V}V_t\dif t + \sigma_v\sqrt{V_t}\left(\rho \dif W_{t,1}^\mathbb{Q} + \sqrt{1-\rho^2}\dif W_{t,2}^\mathbb{Q}\right).\label{QHestonV}
\end{align}
$\left(\dif W_{t,1}^\mathbb{Q}, \dif W_{t,2}^\mathbb{Q}\right)$ are Brownian motions under the (equivalent) martingale pricing measure $\mathbb{Q}$.

Jointly, \ref{QHestonS} and \ref{QHestonV} are known as the Heston model, which features local stochastic volatility with a leverage effect while providing semi-closed form solutions for option prices. This model is a "staple" stochastic volatility model for equity index modeling. It is used in a large number of empirical studies, has a number of well documented limitations, some of which have been remedied by enriching the model specification through the addition of jumps and/or other (latent) stochastic states. 

Given the focus of the study, attention is restricted to the simplistic set-up of the Heston model for a host of reasons. On the one hand, for most Heston model extensions, econometric approaches to parameter estimation which employ option price data come with increased computational costs. Since in the current set-up this model is known to be the data generating process, there's no potential misspecification issue at play.
Secondly, extensions come with specific estimation challenges which typically require tailored estimation designs to obtain well-behaved parameter estimates therefore limiting the comparability of results obtained throughout the study. 

\subsection{Parameter Sets}
Let $\theta$ denote the vector of parameters featured in the model. We assume $\theta \in \Theta$ where $\Theta$ is a compact set from the domain of valid data-generating process parameters. For the Heston model specification previously introduced, the value of $\theta$ corresponding to the data-generating process introduced in \eqref{PHestonS} - \eqref{QHestonV} is $\theta = \left[\eta, \kappa, \bar{v}, \sigma_v, \rho, \eta_v\right]$. Note that subsets of the parameter set play a role in either the $\mathbb{P}$-measure specification, the $\mathbb{Q}$-measure specification or in both. E.g., the Heston parameter set can be decomposed into 3 disjoint subsets: $\theta = \left[\theta^\mathbb{P}, \theta^{\mathbb{P}\cap\mathbb{Q}}, \theta^\mathbb{Q}\right]$, where $\theta^\mathbb{P} = \eta$,~ $\theta^{\mathbb{P}\cap\mathbb{Q}} = \left[\kappa, \bar{v}, \sigma_v, \rho\right]$, and $\theta^\mathbb{Q} = \eta_v$. 

\subsection{State Vector}
Assuming we have a time series of state vector observations:\footnote{In practice such a series never exists as the volatility, $V_t$, is an unobservable, i.e., latent state process} $X_1, \dots, X_T$, observed at  equally-spaced timepoints $t_1,\dots,t_T$ with $t_{i+1} - t_i = \Delta >0, \forall i\geq 1 $. Denote by $f(X_{i+1} |X_i; \theta^\mathbb{P},  \theta^{\mathbb{P} \cap \mathbb{Q}})$ the transition density of the state vector $X_t$ under the historical probability measure. Note that the transition density depends on $\theta^\mathbb{P}$ and $\theta^{\mathbb{P} \cap \mathbb{Q}}$ which represent the subset of parameters that describe the model dynamics under $\mathbb{P}$.

By using option price data the full set of parameters -- including the additional ones that characterize model dynamics under the risk neutral pricing measure -- can be estimated, i.e., inference on $\theta^\mathbb{Q}$ can be conducted while at the same time increasing sample identification potential for the $\theta^{\mathbb{P} \cap \mathbb{Q}}$ parameter set. 

\subsection{Option prices}
Suppose that, in addition to time series observations of returns, synchronous observations of option price panel data and time series data on the underlying are available. Denote the option prices by $p_{i}(T,K)$ where $T,K$ represent the maturity and the strike of the option contract. Option prices are commonly quoted in terms of their Black-Scholes implied volatility. Let $P(X_{i},T,K; \theta^{\mathbb{P}\cap\mathbb{Q}}; \theta^\mathbb{Q})$ be the corresponding model price, e.g., the option price generated by the base model specification under the equivalent martingale pricing measure for  given state vector instance $X_{i}$ and parameter set values $\theta^{\mathbb{P}\cap\mathbb{Q}}$ and $\theta^\mathbb{Q}$. If option prices were observed without error and the model is not misspecified, the observed option prices would coincide with their model-based counterparts. In practice the two prices differ. Typical modelling approaches assume away model misspecification and attribute the difference to measurement errors (e.g., microstructure noise). 

\subsubsection{Particle Filter Based Estimation}
Estimation contexts which make use of option prices (that are nonlinear, often semi-analytic expressions featuring model parameters and partially observable state vectors) can be computationally demanding. The models used to describe index returns for option pricing purposes typically feature latent stochastic state processes which require filtering  or implying. This increases the computational cost of econometric estimation exercises as one has to devise parsimonious estimators which can work with non-analytic expressions of state vector observations by relying on stable and robust numerical procedures. 

%The advent of parallel computing techniques has rendered advanced particle filter methods feasible for set-ups with larger panels of option price observations [references, some of them very new indeed], allowing for the evaluation of (quasi-)likelihood functions of option price observations.

Two other hurdles are typically encountered when working with continuous time models. Firstly, even the simple continuous time stochastic model specifications (like the base-case model used in this paper) do not have known densities readily usable to evaluate the likelihood function. Secondly, one often has to resort to using discretisation schemes to simulate state vector (i.e., particle) paths. However, despite this second hurdle, approximating transition densities using discrete distributions consisting of particle support points provides a tractable workaround which can easily be adapted for most of the typical model specifications considered. The filtering distributions provide estimators for the latent states and their possible distributions. The Monte Carlo procedures involved in the simulation of particles can be tailored to accommodate models with jumps and with more complex state dynamics than featured in the Heston model specification. 

In the context of the base model specification previously introduced, we outline the following optimal filtering procedure in the spirit of \citet{johannes2009optimal}, \citet{christoffersen2010volatility} and ***[Hurn et al]*** and introduce the necessary implementation details for the Monte Carlo estimation exercise. First, let us denote the  log-likelihood function of the model under the physical probability measure by
\begin{equation}
\log \mathcal{L}_T = \log f(X_0) + \sum_{i=0}^T \log f(X_{i+1}|X_{i};\theta^{\mathbb{P},\mathbb{P}\cap\mathbb{Q}}).
\end{equation}
The evaluation of this likelihood function is unfeasible as the state vector is not fully observed. Also note that it does not provide any information about $\theta^\mathbb{Q}$, the parameters which feature in the dynamics of the model under the risk neutral probability measure.
 
To take into account option prices and to allow for the likelihood to be evaluated, a filtering rule using the time series observations for the components of the state vector which are observable is introduced alongside an option price panel likelihood observation in order to extract information about the latent states which are not observable.  Let us denote by $\mathcal{O}_i$ the filtration containing the collection of sigma algebras generated by the time series of observable states and associated option prices. E.g., for our base model specification this is the filtration generated by $S_{1},\dots,S_{T}$ and the panel $p_{1}(T,K),\dots  p_{T}(T,K)$ of option prices for all available option maturities $T$ and strike prices $K$. The level of the stochastic volatility process $V_{i}$ is unobservable. To evaluate the likelihood contribution, the following need to be evaluated:\vspace{+5pt}\\
\texttt{Step 1. $T_{i+1}$ likelihood contribution:} \vspace{-5pt}
\begin{align}
f(X_{i+1}|\mathcal{O}_{i};\theta) &=\int_{\mathcal{D}_{V}}  f(S_{i+1}|{V}_{i+1},\mathcal{O}_i;\theta) f(V_{i+1}|\mathcal{O}_i;\theta) \dif{V_{i+1}} \notag\\
&=\int_{\mathcal{D}_{V}} f(S_{i+1}|{V}_{i+1},\mathcal{O}_i; \theta) \int_{\mathcal{D}_{V}} f(V_{i+1}|{V}_{i},\mathcal{O}_i;\theta) \notag \\
& \hspace{165pt} \times f(V_{i}|\mathcal{O}_i; \theta)  \dif V_{i} \dif{V_{i+1}}\label{LikelihoodContribTheo}
\end{align}

\texttt{Step 2. Latent state variable density update:}\vspace{-5pt}
\begin{align}
f(V_{i+1}|\mathcal{O}_{i+1}; \theta) &= \frac{f(X_{i+1},V_{i+1}|\mathcal{O}_{i+1}; \theta)}{f(X_{i+1}|\mathcal{O}_{i+1}; \theta)} \label{SamplingDensityTheo}
\end{align}
%
%To evaluate \ref{LikelihoodContribTheo} and \ref{SamplingDensityTheo} numerically a combination of Monte Carlo path draws and Monte Carlo integrations is used. Starting with an initial cloud of particles (i.e., a set of likely values for ${V_i}$ pre-sampled from $\mathcal{D}_{V}$) the state vector $X_{i} = \left[S_i, V_i\right]$ is advanced using a discretisation approach (e.g., an Euler scheme). The next step requires calculating $•$

\newpage
\bibliographystyle{apalike}
\nocite{*}
\bibliography{P_ONE_BIB}
\end{document}